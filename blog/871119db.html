<!DOCTYPE html>
<html>
  <!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  
  <title>lua-resty-mlcache 分析 - B</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  
  <meta name="keywords" content=betta-cyber, game, betta, shokill>
  
    <meta name="description" content="betta">
  
  
    <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.02">
  
  
    <link rel="alternate" href="/atom.xml " title="B" type="application/atom+xml">
  

  

  

  <!-- <link rel="stylesheet" href="../css/style.css"> -->
  
<link rel="stylesheet" href="/css/style.css">


  <!--  -->
    <!-- <link rel="stylesheet" href="../css/font-awesome.min.css"> -->
  <!--  -->

  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script> -->
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=0&t=n&d=b9cDq33TdEsfGb_DpY--uB2pjskzQ8TK9LaE3yuIYzE&co=ffffff&cmo=ffffff&cmn=ffffff&ct=ffffff'></script>
<meta name="generator" content="Hexo 5.4.1"></head>

  <body>
    <div class="container">
      <header class="header">
  <!-- <div class="blog-title">
    <a href="/" class="logo">B</a>
    <div class="subtitle"></div>
  </div> -->
  <nav class="navbar">
    <ul class="menu">
      
        <li class="menu-item">
          <a href="/" class="menu-item-link " >Home</a>
        </li>
      
        <li class="menu-item">
          <a href="/blog" class="menu-item-link active" >Blog</a>
        </li>
      
        <li class="menu-item">
          <a href="/ebook" class="menu-item-link " >Ebook</a>
        </li>
      
        <li class="menu-item">
          <a href="/music" class="menu-item-link " >Music</a>
        </li>
      
        <li class="menu-item">
          <a href="/about" class="menu-item-link " >About</a>
        </li>
      
    </ul>
  </nav>
</header>


<article class="post">
  <div class="post-title">
    <h1 class="article-title">lua-resty-mlcache 分析</h1>
  </div>
  <div class="post-content">
    <p>lua-resty-mlcache 用 shared dict 和 lua-resty-lrucache ，实现了多层缓存机制。</p>
<p>mlcache 的架构如下：</p>
<pre><code class="highlight plaintext">┌─────────────────────────────────────────────────┐
│ Nginx                                           │
│       ┌───────────┐ ┌───────────┐ ┌───────────┐ │
│       │worker     │ │worker     │ │worker     │ │
│ L1    │           │ │           │ │           │ │
│       │ Lua cache │ │ Lua cache │ │ Lua cache │ │
│       └───────────┘ └───────────┘ └───────────┘ │
│             │             │             │       │
│             ▼             ▼             ▼       │
│       ┌───────────────────────────────────────┐ │
│       │                                       │ │
│ L2    │           lua_shared_dict             │ │
│       │                                       │ │
│       └───────────────────────────────────────┘ │
│                           │ mutex               │
│                           ▼                     │
│                  ┌──────────────────┐           │
│                  │     callback     │           │
│                  └────────┬─────────┘           │
└───────────────────────────┼─────────────────────┘
                            │
  L3                        │   I/O fetch
                            ▼

                   Database, API, DNS, Disk, any I/O...</code></pre>

<p>分为三层：</p>
<p>L1 层是使用 lua-resty-lrucache 的虚拟缓存。提供最快的查找。</p>
<p>L2 层是用的 Nginx 的 lua_shared_dict 内存共享。当 L1 层的 key miss 掉时，从这里获取。</p>
<p>L3 层提供一个自定义函数，有一个 Worker 执行，通过 lua-resty-lock 避免对后端的多次访问。L3 获取的值会被放到 L2 缓存当中。</p>
<p>用来做缓存的组件，shared dict 缓存和 lru 缓存。前者只能缓存字符串对象，缓存的数据有且只有一份，每一个 worker 都可以进行访问，所以常用于 worker 之间的数据通信。后者则可以缓存所有的 Lua 对象，但只能在单个 worker 进程内访问，有多少个 worker，就会有多少份缓存数据。</p>
<h2 id="缓存有两个原则"><a href="#缓存有两个原则" class="headerlink" title="缓存有两个原则"></a>缓存有两个原则</h2><p>一是越靠近用户的请求越好，比如能用本地缓存的就不要发送HTTP请求，能用CDN缓存的就不要打到web服务器，能用nginx缓存的就不要用数据库的缓存。</p>
<p>二是尽量使用本进程和本机的缓存解决，因为跨了进程和机器甚至机房，缓存的网络开销就会非常大，在高并发的时候会非常明显。</p>
<p>我们直接贴一下官方的示例</p>
<pre><code class="highlight plaintext"># nginx.conf

http &#123;
    lua_package_path &quot;/path/to/lua-resty-mlcache/lib/?.lua;;&quot;;
    lua_shared_dict cache_dict 1m;

    init_by_lua_block &#123;
        local mlcache = require &quot;resty.mlcache&quot;

        local cache, err = mlcache.new(&quot;my_cache&quot;, &quot;cache_dict&quot;, &#123;
            lru_size = 500,    -- size of the L1 (Lua VM) cache
            ttl      = 3600,   -- 1h ttl for hits
            neg_ttl  = 30,     -- 30s ttl for misses
        &#125;)
        if err then

        end

        _G.cache = cache
    &#125;

    server &#123;
        listen 8080;
        location / &#123;
            content_by_lua_block &#123;
                local function callback(username)
                    return db:get_user(username) -- &#123; name = &quot;John Doe&quot;, email = &quot;john@example.com&quot; &#125;
                end

                local user, err = cache:get(&quot;my_key&quot;, nil, callback, &quot;John Doe&quot;)
                ngx.say(user.username) -- &quot;John Doe&quot;
            &#125;
        &#125;
    &#125;
&#125;</code></pre>

<p>以上的示例很好了描述了整个程序运行的逻辑，在init阶段初始化缓存，然后用_G变量赋予全局变量，在使用阶段cache:get获取指定Key的缓存，缓存未命中就会调用L3，也就是callback方法。</p>
<h2 id="缓存风暴问题"><a href="#缓存风暴问题" class="headerlink" title="缓存风暴问题"></a>缓存风暴问题</h2><p>但这个示例中缺失了lua-resty-lock这个组件的调用，为了防止在L3阶段发现缓存风暴，所以把锁非常有必要。</p>
<p>将局部配置修改如下：</p>
<pre><code class="highlight plaintext">lua_shared_dict cache_dict 1m;
lua_shared_dict cache_lock 1m;

init_by_lua_block &#123;
    local mlcache = require &quot;resty.mlcache&quot;

    local cache, err = mlcache.new(&quot;my_cache&quot;, &quot;cache_dict&quot;, &#123;
        lru_size = 500,    -- size of the L1 (Lua VM) cache
        ttl      = 3600,   -- 1h ttl for hits
        neg_ttl  = 30,     -- 30s ttl for misses
        shm_locks = &quot;cache_lock&quot;,
        resty_lock_opts = &#123;
            exptime = 10,
            timeout = 5
        &#125;
    &#125;)
    if err then

    end
&#125;</code></pre>

<h2 id="进程之间通讯问题"><a href="#进程之间通讯问题" class="headerlink" title="进程之间通讯问题"></a>进程之间通讯问题</h2><p>这个问题我们使用lua-resty-worker-events模块解决。</p>
<p>此模块提供了一种向Nginx服务器中的其他工作进程发送事件的方法。通信是通过一个共享的存储区进行的，事件数据将存储在该存储区中。</p>
<p>结合我们之前的缓存使用场景，在一个Worker中的缓存更新之后，要通知其他Worker也同步更新，它就发挥作用了。</p>
<p>我们看以下官方提供的示例</p>
<pre><code class="highlight plaintext">lua_shared_dict process_events 1m;

init_worker_by_lua_block &#123;
    local ev = require &quot;resty.worker.events&quot;

    local handler = function(data, event, source, pid)
        print(&quot;received event; source=&quot;,source,
                &quot;, event=&quot;,event,
                &quot;, data=&quot;, tostring(data),
                &quot;, from process &quot;,pid)
    end

    ev.register(handler)

    local ok, err = ev.configure &#123;
        shm = &quot;process_events&quot;, -- defined by &quot;lua_shared_dict&quot;
        timeout = 2,            -- life time of unique event data in shm
        interval = 1,           -- poll interval (seconds)

        wait_interval = 0.010,  -- wait before retry fetching event data
        wait_max = 0.5,         -- max wait time before discarding event
        shm_retries = 999,      -- retries for shm fragmentation (no memory)
    &#125;
    if not ok then
        ngx.log(ngx.ERR, &quot;failed to start event system: &quot;, err)
        return
    end
&#125;</code></pre>

<p>在init_worker_by_lua_block阶段初始化，是因为它需要在每个Worker中都运行，便于同步到其他进程，其他的就是一些配置参数问题。</p>
<p>下面我们把它结合上面的缓存模块一起使用。</p>
<p>lua-resty-mlcache提供了ipc接口来支持lua-resty-worker-events模块，我们直接配置参数即可。</p>
<pre><code class="highlight plaintext">lua_shared_dict cache_dict    1m;
lua_shared_dict cache_lock    1m;
lua_shared_dict worker_events 1m;

init_worker_by_lua_block &#123;
    local mlcache = require &quot;resty.mlcache&quot;
    local worker_events = require &quot;resty.worker.events&quot;

    local ok, err = worker_events.configure &#123;
        shm = &quot;worker_events&quot;, 
        timeout = 2,           
        interval = 1,    

        wait_interval = 0.010, 
        wait_max = 0.5,  
        shm_retries = 999,  
    &#125;

    local cache, err = mlcache.new(&quot;my_cache&quot;, &quot;cache_dict&quot;, &#123;
        lru_size = 500,    -- size of the L1 (Lua VM) cache
        ttl      = 3600,   -- 1h ttl for hits
        neg_ttl  = 30,     -- 30s ttl for misses
        shm_locks = &quot;cache_lock&quot;,
        resty_lock_opts = &#123;
            exptime = 10,
            timeout = 5
        &#125;,
        ipc = &#123;
            register_listeners = function(events)
                for _, event_t in pairs(events) do
                    worker_events.register(
                        function(data)
                            event_t.handler(data)
                        end,
                        channel_name,
                        event_t.channel
                    )
                end
            end,
            broadcast = function(channel, data)
                worker_events.post(channel_name, channel, data)
            end
        &#125;
    &#125;)
    if err then

    end
&#125;</code></pre>


  </div>
  <div class="post-footer">
    

    <a href="#top" class="top">↑</a>
  </div>
</article>
<footer>
  &copy; 2023
  <span class="author">
    betta
  </span>
</footer>

    </div>
  </body>
</html>
